## 1. 什么是欠拟合和过拟合
先看三张图片，这三张图片是线性回归模型 拟合的函数和训练集的关系

1. 第一张图片拟合的函数和训练集误差较大，我们称这种情况为 `欠拟合`
2. 第二张图片拟合的函数和训练集误差较小，我们称这种情况为 `合适拟合`
3. 第三张图片拟合的函数完美的匹配训练集数据，我们称这种情况为 `过拟合`

![](http://52opencourse.com/?qa=blob&qa_blobid=14751694499845949021)
![](http://52opencourse.com/?qa=blob&qa_blobid=12431723891296509683)
![](http://52opencourse.com/?qa=blob&qa_blobid=6708796742440812238)

类似的，对于逻辑回归同样也存在欠拟合和过拟合问题，如下三张图

![](http://52opencourse.com/?qa=blob&qa_blobid=17500061470523325095)
![](http://52opencourse.com/?qa=blob&qa_blobid=4878699863271022498)
![](http://52opencourse.com/?qa=blob&qa_blobid=5605822270153742742)

## 2. 如何解决欠拟合和过拟合问题
欠拟合问题，根本的原因是特征维度过少，导致拟合的函数无法满足训练集，误差较大。

欠拟合问题可以通过增加特征维度来解决

过拟合问题，根本的原因则是特征维度过多，导致拟合的函数完美的经过训练集，但是对新数据的预测结果则较差。

解决过拟合问题，则有2个途径

1. 减少特征维度; 可以人工选择保留的特征，或者模型选择算法
2. 正则化; 保留所有的特征，通过降低参数θ的值，来影响模型

## 3. 正则化
回到前面`过拟合`例子, h(x) = θ0 + θ1*x1 + θ2*x2 + θ3*x3 + θ4*x4

![](http://52opencourse.com/?qa=blob&qa_blobid=6708796742440812238)
![](http://52opencourse.com/?qa=blob&qa_blobid=12431723891296509683)

从图中可以看出，解决这个过拟合问题可以通过消除特征x3和x4的影响, 我们称为对参数的`惩罚`, 也就是使得参数θ3, θ4接近于0。

最简单的方法是对代价函数进行改造，例如

![](http://52opencourse.com/?qa=blob&qa_blobid=14870802024620705686)

这样在求解最小化代价函数的时候使得参数θ3, θ4接近于0。

`正则化`其实就是通过对参数θ的惩罚来影响整个模型

## 4. 线性回归使用正则化
前面几篇文章中，线性回归的`代价函数`J(θ)表达式如下

![](https://camo.githubusercontent.com/69d7473a15e3ebc5f447bdf7d3091cc2eb0a4f8e/687474703a2f2f696d672e626c6f672e6373646e2e6e65742f3230313630343138313931333030333836)

正则化后，代价函数J(θ)表达式如下，注意j从1开始

![](http://52opencourse.com/?qa=blob&qa_blobid=5037788228953872473)

注意λ值不能设置过大，否则会导致求出的参数除了θ0，其它θ1,θ2 ... θn值约等于0，导致预测函数h(x)出现极大偏差

我们的目标依然是求J(θ)最小值，我们还是用`梯度下降算法`和`正规方程`求解最小化J(θ)

#### 1. 梯度下降算法(注意需要区分θ0和其它参数的更新等式)

![](http://52opencourse.com/?qa=blob&qa_blobid=12566518656416525815)

![](http://images.cnitblog.com/blog/575572/201311/09090536-29b9a7d7547b4080b5d405c62c521cf5.png)

#### 2. 正规方程
对于正规方程来，需要修改等式如下

![](http://52opencourse.com/?qa=blob&qa_blobid=3138763398201409294)

系数λ 所乘的矩阵为 (n+1)*(n+1)维

## 5. 逻辑回归使用正则化
和线性回归模型类型，逻辑回归也可以通过正则化来解决过拟合问题。

逻辑回归的`代价函数`J(θ)表达式如下

![](https://camo.githubusercontent.com/aef6db6086e6e651fdccb46e9c2b1f2fb72031f8/687474703a2f2f696d672e69743631302e636f6d2f696d6167652f696e666f352f65373734333062313463663534346564383862343239346166393236623663352e706e67)

正则化逻辑回归的代价函数，是在等式后加上一项，注意j从1开始

![](http://images.cnitblog.com/blog/575572/201311/09090741-a1c2933e3f664f6bb6084eeedcda7f86.png)

同样的用`梯度下降算法`求解最小化J(θ)，也需要做改变

![](http://52opencourse.com/?qa=blob&qa_blobid=17817426101792596197)

![](http://images.cnitblog.com/blog/575572/201311/09090536-29b9a7d7547b4080b5d405c62c521cf5.png)

不同的是逻辑回归模型中的预测函数 h(x)和线性回归不同
