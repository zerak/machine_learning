## 1. 特征缩放
实际当我们在计算线性回归模型的时候，会发现特征变量x，不同维度之间的取值范围差异很大。这就造成了我们在使用梯度下降算法的时候，由于维度之间的差异使得Jθ的值收敛的很慢。

我们还是以房价预测为例子，我们使用2个特征。房子的尺寸（1~2000），房间的数量（1-5）。以这两个参数为横纵坐标，绘制代价函数的等高线图能看出整个图显得很扁，假如红色的轨迹即为函数收敛的过程，会发现此时函数收敛的非常慢。

![](http://img.blog.csdn.net/20160418193311664)

为了解决这个问题，我们采用`特征缩放`,所谓的特征缩放就是把所有的特征都缩放到一个相近的取值范围内。比如-1~1，或者-0.5~2，或者-2~05.等等，职业不超过-3 ~ 3基本上都能够满足梯度下降算法

最简单的方法采用下面的公式进行计算

![](http://img.blog.csdn.net/20160418193448508)

1. Xn表示第n个特征，也就是特征变量X的第n维
2. Un表示特征的平均值，也就是第n个特征的平均值
3. Sn表示标准差，也就是第n个特征中最大值和最小值的差

实际上，当我们在运用线性回归时，不一定非要直接用给出的 x1, x2, x3 ... xn 作为特征，有时候可以自己创造新的特征。 比如训练集中只给了房子长度和宽度两个特征，但是我们可以用长度X宽度得到面积这个新的特征。 
有时，通过定义新的特征，可以得到一个更好的模型。

## 2. 学习速率
梯度下降算法中，最合适即每次跟着参数θ变化的时候，J(θ)的值都应该下降
到目前为止，我们还没有介绍如何选择学历速率α，梯度下降算法每次迭代，都会受到学习速率α的影响

1. 如果α较小，则达到收敛所需要迭代的次数就会非常高；
2. 如果α较大，则每次迭代可能不会减小代价函数的结果，甚至会超过局部最小值导致无法收敛。
   ![](https://camo.githubusercontent.com/1b6f2c394d39a7c057a5726c7e1b3ce6ee9c6362/687474703a2f2f696d672e6d792e6373646e2e6e65742f75706c6f6164732f3230313230392f30362f313334363930323330305f343137392e706e67)

观察下图，可以发现这2种情况下代价函数 J(θ)的迭代都不是正确的

![](http://images.cnitblog.com/blog/663864/201410/272201153783110.png)

1. 第一个图，曲线在上升，明显J(θ)的值变得越来越大，说明应该选择较小的α
2. 第二个图，J(θ)的曲线，先下降，然后上升，接着又下降，然后又上升，如此往复。通常解决这个问题，还是选取较小的α

根据经验，可以从以下几个数值开始试验α的值，0.001 ,0.003, 0.01, 0.03, 0.1, 0.3, 1, …
α初始值位0.001, 不符合预期乘以3倍用0.003代替，不符合预期再用0.01替代，如此循环直至找到最合适的α
然后对于这些不同的 α 值，绘制 J(θ)随迭代步数变化的曲线，然后选择看上去使得 J(θ)快速下降的一个 α 值。

所以，在为梯度下降算法选择合适的学习速率α 时，可以大致按3的倍数再按10的倍数来选取一系列α值，直到我们找到一个值它不能再小了，同时找到另一个值，它不能再大了。其中最大的那个 α 值，或者一个比最大值略小一些的α 值 就是我们期望的最终α 值。
