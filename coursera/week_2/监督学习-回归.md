##一.引言
机器学习中的回归问题属于有监督学习的范畴，回归问题的目标是给定D维输入变量x，并且每一个输入矢量x都有对应的值y，

要求对于新来的数据预测它对应的连续的目标值t。

比如下面这个例子：假设我们有一个包含47个房子的面积和价格的数据集如下：
![](http://images.cnitblog.com/blog/392228/201410/291919410655805.jpg)

我们可以在Matlab中画出来这组数据集，如下：
![](http://images.cnitblog.com/blog/392228/201410/291921072538240.jpg)

看到画出来的点，是不是有点像一条曲线？
我们可以用一条曲线去尽量拟合这些数据点，那么对于新来的输入，我么就可以将拟合的曲线上返回对应的点从而达到预测的目的。
如果要预测的值是连续的比如上述的房价，那么就属于回归问题；如果要预测的值是离散的即一个个标签，那么就属于分类问题。

这个学习处理过程如下图所示:

![](http://images.cnitblog.com/blog/392228/201410/291925279255104.jpg)

上述学习过程中的常用术语：
1. 包含房子面积和价格的数据集称为 "训练集training set" 
2. 输入变量x（本例中为面积）为 "特征features"
3. 输出的预测值y（本例中为房价）为 "目标值target"
4. 拟合的曲线，一般表示为y = h(x)，称为 "假设模型hypothesis"

##二.线性回归模型
线性回归模型假设输入特征和对应的结果满足线性关系。
在上述的数据集中加上一维--房间数量，于是数据集变为：

![](http://images.cnitblog.com/blog/392228/201410/291942173624782.jpg)

于是，输入特征x是二维的矢量，比如x1(i)表示数据集中第i个房子的面积，x2(i)表示数据集中第i个房子的房间数量。

于是可以假设输入特征x与房价y满足线性函数，比如：

![](http://images.cnitblog.com/blog/392228/201410/291946319404707.jpg)

这里θi称为假设模型即映射输入特征x与结果y的线性函数h的参数parameters，为了简化表示，我们在输入特征中加入x0 = 1，
于是得到：
参数θ和输入特征x都为矢量，n是输入的特征x的维数（不包含x0）

![](http://images.cnitblog.com/blog/392228/201410/291951223156962.jpg)

现在，给定一个训练集，我们应该怎么学习参数θ，从而达到比较好的拟合效果呢？一个直观的想法是使得预测值h(x)尽可能接近y。
一种方法是计算它的成本函数(Cost function),即预测出来的值与实际值y之间的方差的大小来决定当前值是否是最优的！

为什么选择方差而不是选择偏差呢？
因为偏差有正有负会相互抵消，因此选择方差更能准确表示两者之间的差异。
因此我们对于每一个参数θ，定义一个代价函数cost function用来描述h(x(i))'与对应的y(i)'的接近程度：
（假设现在只有2个参数θ0和θ1）

![](http://studentdeng.github.io/images/ml/12.png)

前面乘上的1/2m是为了求导的时候，使常数系数消失。于是我们的目标就变为了调整θ使得代价函数J(θ)取得最小值，
方法有梯度下降法，最小二乘法等。

